{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_xmyQbjypHC9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import PIL\n",
        "\n",
        "from custom_dataset import CustomDataset\n",
        "\n",
        "\n",
        "# class CustomDataset(Dataset):\n",
        "#     def __init__(self, image_path_to_class_dict, class_to_number_dict, apply_resize=False):\n",
        "#         super().__init__()\n",
        "#         self.image_path_to_class_dict = image_path_to_class_dict\n",
        "#         self.class_to_number_dict = class_to_number_dict\n",
        "#         self.image_paths = list(self.image_path_to_class_dict.keys())\n",
        "#         self.resize = transforms.Resize((224, 224))  # Resize to 256x256 for ViT input\n",
        "#         self.to_tensor = transforms.ToTensor()  # Converts images to tensor\n",
        "#         self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "#                                               std=[0.229, 0.224, 0.225])  # Standard normalization for ViT\n",
        "#         self.apply_resize = apply_resize\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.image_paths)\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         # Open the image and convert to RGB\n",
        "#         image = PIL.Image.open(self.image_paths[index]).convert('RGB')\n",
        "\n",
        "#         # Apply resize if needed\n",
        "#         if self.apply_resize:\n",
        "#             image = self.resize(image)\n",
        "\n",
        "#         # Convert the image to a PyTorch tensor and normalize\n",
        "#         image = self.to_tensor(image)\n",
        "#         image = self.normalize(image)\n",
        "\n",
        "#         # Get the corresponding label\n",
        "#         class_name = self.image_path_to_class_dict[self.image_paths[index]]\n",
        "#         label = self.class_to_number_dict[class_name]\n",
        "\n",
        "#         # Convert label to tensor\n",
        "#         label = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "#         return image, label\n",
        "    \n",
        "#     if __name__ == \"__main__\":\n",
        "#         dataset = CustomDataset([1, 2, 3, 4, 5])\n",
        "#         dataloader = DataLoader(dataset, num_workers=2) \n",
        "#         for batch in dataloader:\n",
        "#             print(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22\n"
          ]
        }
      ],
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "our_data_simple = ImageFolder(\"train\",\n",
        "                              transform=transforms.Compose([transforms.Resize((256,256)), transforms.ToTensor()]),\n",
        "                              target_transform=None)\n",
        "\n",
        "class_to_number_dict = our_data_simple.class_to_idx\n",
        "\n",
        "print(len(class_to_number_dict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zlK1HLc8o6cX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mps\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import timm\n",
        "from torchvision.models import vit_b_16\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "        num_classes = len(class_to_number_dict)\n",
        "        # Load the pre-trained ViT model\n",
        "        self.vit_model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "\n",
        "        # Replace the classification head (final layer) to match the number of classes\n",
        "        self.vit_model.head = nn.Linear(self.vit_model.head.in_features, num_classes)\n",
        "\n",
        "        # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "\n",
        "        self.vit_model.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the Vision Transformer model\n",
        "        return self.vit_model(x)\n",
        "    \n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAxRS8z-p3Tl",
        "outputId": "4c6fc236-3841-4428-ad17-147b8a1052bf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# parser = argparse.ArgumentParser(description= \\\n",
        "#                                      'scipt for training of project 2')\n",
        "# parser.add_argument('--cuda', action='store_true', default=False,\n",
        "#                     help='Used when there are cuda installed.')\n",
        "# args = parser.parse_args()\n",
        "\n",
        "# training process.\n",
        "def train_net(net, trainloader, valloader):\n",
        "########## ToDo: Your codes goes below #######\n",
        "    val_accuracy = 0\n",
        "    # val_accuracy is the validation accuracy of each epoch. You can save your model base on the best validation accuracy.\n",
        "\n",
        "    # Define the loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = Adam(network.parameters(), lr=0.0001)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "\n",
        "    # optimizer = Adam(network.parameters(), lr=0.001)\n",
        "\n",
        "    num_epochs = 20\n",
        "    best_val_acc = 0.0\n",
        "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "\n",
        "    net.to(device)\n",
        "\n",
        "    # Training loop\n",
        "    # Implemented early stopping based on validation accuracy, saved the model at its best validation performance, and added a learning rate scheduler.\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(trainloader)}\")\n",
        "        print(f\"Training Accuracy: {100 * correct_train / total_train}%\")\n",
        "\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(trainloader)}, Training Accuracy: {train_accuracy}%\")\n",
        "\n",
        "\n",
        "    # Validation loop\n",
        "    net.eval()\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for data in valloader:\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = net(inputs)\n",
        "            val_loss += criterion(outputs, labels).item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_accuracy = 100 * correct_val / total_val\n",
        "    print(f\"Validation Accuracy: {val_accuracy}%\")\n",
        "\n",
        "    # Save the model if validation accuracy improves\n",
        "    if val_accuracy > best_val_acc:\n",
        "        best_val_acc = val_accuracy\n",
        "        torch.save(net.state_dict(), 'best_model.pth')\n",
        "        print(f\"Best model saved with validation accuracy: {best_val_acc}%\")\n",
        "\n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "    return best_val_acc\n",
        "    # return val_accuracy\n",
        "\n",
        "##############################################\n",
        "\n",
        "############################################\n",
        "# Transformation definition\n",
        "# NOTE:\n",
        "# Write the train_transform here. We recommend you use\n",
        "# Normalization, RandomCrop and any other transform you think is useful.\n",
        "\n",
        "# As we know that data augmentation helps improve the model's robustness. Used a wider variety of transformations, such as random rotation, color jitter, and affine transformations.\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalization\n",
        "])\n",
        "\n",
        "\n",
        "# train_transform = transforms.Compose([\n",
        "#     transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "#     transforms.RandomHorizontalFlip(), # to be toggled for testing\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "# ])\n",
        "####################################\n",
        "\n",
        "####################################\n",
        "# Define the training dataset and dataloader.\n",
        "# You can make some modifications, e.g. batch_size, adding other hyperparameters, etc.\n",
        "\n",
        "# Folder path containing the dataset (e.g., '../dataset/')\n",
        "root = 'train'  # Replace with the path to your dataset\n",
        "\n",
        "classes = os.listdir(root)\n",
        "\n",
        "!rm -rf /train/.DS_Store\n",
        "\n",
        "# register all images with their corresponding classes\n",
        "image_path_to_class_dict = {}\n",
        "for class_name in classes:\n",
        "  images = os.listdir(os.path.join(root, class_name))\n",
        "  for image in images:\n",
        "    image_path_to_class_dict[os.path.join(root, class_name, image)] = class_name\n",
        "\n",
        "our_data_simple = ImageFolder(root,\n",
        "                              transform=transforms.Compose([transforms.Resize((256,256)), transforms.ToTensor()]),\n",
        "                              target_transform=None)\n",
        "\n",
        "class_to_number_dict = our_data_simple.class_to_idx\n",
        "\n",
        "# Create an instance of the custom dataset\n",
        "custom_dataset = CustomDataset(image_path_to_class_dict, class_to_number_dict, apply_resize=True)\n",
        "\n",
        "# Split the dataset into train and validation sets (80% train, 20% val)\n",
        "train_size = int(0.8 * len(custom_dataset))\n",
        "val_size = len(custom_dataset) - train_size\n",
        "\n",
        "trainset, valset = torch.utils.data.random_split(custom_dataset, [train_size, val_size])\n",
        "\n",
        "\n",
        "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
        "#                                          shuffle=True, num_workers=4)\n",
        "# valloader = torch.utils.data.DataLoader(valset, batch_size=32,\n",
        "#                                          shuffle=True, num_workers=4)\n",
        "\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
        "                                         shuffle=True, num_workers=4)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=64,\n",
        "                                         shuffle=False, num_workers=4)\n",
        "####################################\n",
        "\n",
        "# ==================================\n",
        "# use cuda if called with '--cuda'.\n",
        "\n",
        "network = Network()\n",
        "# if args.cuda:\n",
        "#     network = network.cuda()\n",
        "\n",
        "# network = network.cuda()\n",
        "\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "network = network.to(device)\n",
        "\n",
        "\n",
        "# train and eval your trained network\n",
        "# you have to define your own\n",
        "val_acc = train_net(network, trainloader, valloader)\n",
        "\n",
        "print(\"final validation accuracy:\", val_acc)\n",
        "\n",
        "# =================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'CustomDataset' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 111\u001b[0m\n\u001b[1;32m    108\u001b[0m         image_path_to_class_dict[os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, class_name, image)] \u001b[38;5;241m=\u001b[39m class_name\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Create an instance of the custom dataset\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m custom_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCustomDataset\u001b[49m(image_path_to_class_dict, class_to_number_dict, apply_resize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Split the dataset into train and validation sets\u001b[39;00m\n\u001b[1;32m    114\u001b[0m train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.8\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(custom_dataset))\n",
            "\u001b[0;31mNameError\u001b[0m: name 'CustomDataset' is not defined"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "# CustomDataset class needs to be defined before this\n",
        "# Example: \n",
        "# class CustomDataset(torch.utils.data.Dataset):\n",
        "#     pass  # Your custom dataset implementation\n",
        "\n",
        "# Training process with additional tracking\n",
        "def train_net(net, trainloader, valloader):\n",
        "    # Define the loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = Adam(net.parameters(), lr=0.0001)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "    num_epochs = 20\n",
        "    best_val_acc = 0.0\n",
        "    prev_val_acc = 0.0  # Store the validation accuracy of the previous epoch\n",
        "    net.to(device)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        train_accuracy = 100 * correct_train / total_train\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(trainloader)}, Training Accuracy: {train_accuracy}%\")\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = net(inputs)\n",
        "                val_loss += criterion(outputs, labels).item()\n",
        "\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total_val += labels.size(0)\n",
        "                correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "        val_accuracy = 100 * correct_val / total_val\n",
        "        print(f\"Validation Accuracy: {val_accuracy}%\")\n",
        "\n",
        "        # Compare training and validation accuracy\n",
        "        if val_accuracy < prev_val_acc and train_accuracy > prev_train_acc:\n",
        "            print(f\"Warning: Validation accuracy decreased while training accuracy increased at epoch {epoch+1}\")\n",
        "\n",
        "        prev_val_acc = val_accuracy  # Store current validation accuracy for the next comparison\n",
        "        prev_train_acc = train_accuracy  # Store current training accuracy for the next comparison\n",
        "\n",
        "        # Save the model if validation accuracy improves\n",
        "        if val_accuracy > best_val_acc:\n",
        "            best_val_acc = val_accuracy\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "            print(f\"Best model saved with validation accuracy: {best_val_acc}%\")\n",
        "\n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "    return best_val_acc\n",
        "\n",
        "\n",
        "# ==================================\n",
        "# Define the training dataset and dataloader.\n",
        "root = 'train'  # Replace with the path to your dataset\n",
        "\n",
        "# Folder path containing the dataset\n",
        "classes = os.listdir(root)\n",
        "\n",
        "# Remove hidden system files if present\n",
        "!rm -rf /train/.DS_Store\n",
        "\n",
        "# Custom dataset registration\n",
        "image_path_to_class_dict = {}\n",
        "for class_name in classes:\n",
        "    images = os.listdir(os.path.join(root, class_name))\n",
        "    for image in images:\n",
        "        image_path_to_class_dict[os.path.join(root, class_name, image)] = class_name\n",
        "\n",
        "# Create an instance of the custom dataset\n",
        "custom_dataset = CustomDataset(image_path_to_class_dict, class_to_number_dict, apply_resize=True)\n",
        "\n",
        "# Split the dataset into train and validation sets\n",
        "train_size = int(0.8 * len(custom_dataset))\n",
        "val_size = len(custom_dataset) - train_size\n",
        "trainset, valset = torch.utils.data.random_split(custom_dataset, [train_size, val_size])\n",
        "\n",
        "# Dataloaders for training and validation\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=4)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "# ==================================\n",
        "# Initialize network and train\n",
        "network = Network()\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "network = network.to(device)\n",
        "\n",
        "# Train and evaluate\n",
        "val_acc = train_net(network, trainloader, valloader)\n",
        "print(\"Final validation accuracy:\", val_acc)\n",
        "# ==================================\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
